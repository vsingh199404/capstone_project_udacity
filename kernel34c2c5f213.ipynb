{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nSEED=2\nimport os\n\ntrain=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/train.csv\")\nsubmition=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/test.csv\")\n\nx = train['id_code']\ny = train['diagnosis']\n\nx,y=shuffle(x,y)\n# Any results you write to the current directory are saved as output.","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X,X_test,df_y,y_test=train_test_split(x, y, test_size=0.15)\n\nX_train,X_valid,y_train,y_valid=train_test_split(df_X, df_y, test_size=0.15)\n","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_ben_color(path, sigmaX=20):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n        \n    return image","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\ntrain_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_train):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    train_images.append(image)\n\n        ","execution_count":93,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 8.58 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\ntest_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_test):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    test_images.append(image)\n","execution_count":94,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 8.82 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\nvalid_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_valid):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    valid_images.append(image)","execution_count":95,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 9.06 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ny_train_dummies=to_categorical(y_train)\ny_test_dummies=to_categorical(y_test)\ny_valid_dummies=to_categorical(y_valid)\ny_train_dummies","execution_count":155,"outputs":[{"output_type":"execute_result","execution_count":155,"data":{"text/plain":"array([[1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       ...,\n       [1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport time\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers import merge, Input\nfrom keras.models import Model\nfrom keras.utils import np_utils\nimport os\n\n\n","execution_count":156,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_Classes=5\n\ntrain=np.array(train_images)\ntest=np.array(test_images)\nvalid=np.array(valid_images)\n\nnum_of_samples=train.shape[0]\ntrain.shape","execution_count":157,"outputs":[{"output_type":"execute_result","execution_count":157,"data":{"text/plain":"(2343, 224, 224, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 4\n\n# Add Image augmentation to our generator\ndatagen=ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n","execution_count":158,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image                  \nimage_input=Input(shape=train.shape[1:])\nmodel=VGG16(input_tensor=image_input, weights='imagenet')\nmodel.summary()","execution_count":159,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlast_layer=model.get_layer('fc1').output\nlast_layer=Dropout(0.2)(last_layer)\nout=Dense(num_Classes,activation='softmax', name='output')(last_layer)\n\ncustom_vgg_model=Model(image_input,out)\ncustom_vgg_model.summary()","execution_count":160,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\noutput (Dense)               (None, 5)                 20485     \n=================================================================\nTotal params: 117,499,717\nTrainable params: 117,499,717\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in custom_vgg_model.layers[:-2]:\n    layer.trainable=False","execution_count":161,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_vgg_model.compile(loss=\"categorical_crossentropy\", optimizer='Adam',  metrics=['accuracy'])","execution_count":162,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndatagen.fit(train)\n\nt=time.time()\n\ncustom_vgg_model.fit_generator(datagen.flow(train,y_train_dummies, batch_size=32),steps_per_epoch=len(X_train), epochs=3, verbose=1 ,validation_data=(valid,y_valid_dummies))\n\nprint(\"training time: %s\" %(t-time.time()))","execution_count":163,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n2343/2343 [==============================] - 717s 306ms/step - loss: 0.8785 - acc: 0.7026 - val_loss: 4.0367 - val_acc: 0.6553\nEpoch 2/3\n2343/2343 [==============================] - 713s 304ms/step - loss: 0.7644 - acc: 0.7384 - val_loss: 5.4120 - val_acc: 0.6109\nEpoch 3/3\n2343/2343 [==============================] - 715s 305ms/step - loss: 0.7457 - acc: 0.7463 - val_loss: 6.4427 - val_acc: 0.5648\ntraining time: -2145.83038187027\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nt=time.time()\n\ncustom_vgg_model.fit(train,y_train_dummies, batch_size=32, epochs=30, verbose=1 ,validation_data=(valid,y_valid_dummies))\n\nprint(\"training time: %s\" %(t-time.time()))","execution_count":164,"outputs":[{"output_type":"stream","text":"Train on 2343 samples, validate on 586 samples\nEpoch 1/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 4.2548 - acc: 0.7017 - val_loss: 4.2430 - val_acc: 0.7167\nEpoch 2/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 4.0679 - acc: 0.7217 - val_loss: 3.8921 - val_acc: 0.7321\nEpoch 3/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.7460 - acc: 0.7452 - val_loss: 3.6728 - val_acc: 0.7526\nEpoch 4/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.5540 - acc: 0.7571 - val_loss: 3.5269 - val_acc: 0.7628\nEpoch 5/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.4531 - acc: 0.7601 - val_loss: 3.5515 - val_acc: 0.7577\nEpoch 6/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.4114 - acc: 0.7644 - val_loss: 3.3427 - val_acc: 0.7713\nEpoch 7/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.3246 - acc: 0.7644 - val_loss: 3.3430 - val_acc: 0.7611\nEpoch 8/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.5464 - acc: 0.7593 - val_loss: 3.5249 - val_acc: 0.7628\nEpoch 9/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.4562 - acc: 0.7631 - val_loss: 3.7257 - val_acc: 0.7491\nEpoch 10/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.1291 - acc: 0.7776 - val_loss: 3.4071 - val_acc: 0.7594\nEpoch 11/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 3.0876 - acc: 0.7776 - val_loss: 3.4451 - val_acc: 0.7628\nEpoch 12/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.8250 - acc: 0.7900 - val_loss: 3.2307 - val_acc: 0.7765\nEpoch 13/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.7065 - acc: 0.7956 - val_loss: 2.9690 - val_acc: 0.7833\nEpoch 14/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.5707 - acc: 0.8135 - val_loss: 3.2393 - val_acc: 0.7799\nEpoch 15/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.4985 - acc: 0.8139 - val_loss: 2.9988 - val_acc: 0.7594\nEpoch 16/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.4312 - acc: 0.8199 - val_loss: 2.9028 - val_acc: 0.7867\nEpoch 17/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.4564 - acc: 0.8160 - val_loss: 2.7768 - val_acc: 0.7901\nEpoch 18/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.2617 - acc: 0.8306 - val_loss: 2.8803 - val_acc: 0.7799\nEpoch 19/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.2789 - acc: 0.8310 - val_loss: 2.8280 - val_acc: 0.7833\nEpoch 20/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.2874 - acc: 0.8246 - val_loss: 3.2916 - val_acc: 0.7645\nEpoch 21/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.1839 - acc: 0.8344 - val_loss: 3.2301 - val_acc: 0.7765\nEpoch 22/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.0784 - acc: 0.8365 - val_loss: 2.7820 - val_acc: 0.7952\nEpoch 23/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.2723 - acc: 0.8250 - val_loss: 3.0098 - val_acc: 0.7662\nEpoch 24/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.2055 - acc: 0.8293 - val_loss: 2.8215 - val_acc: 0.7918\nEpoch 25/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.0554 - acc: 0.8365 - val_loss: 3.1129 - val_acc: 0.7782\nEpoch 26/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 2.0650 - acc: 0.8399 - val_loss: 2.6974 - val_acc: 0.7884\nEpoch 27/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 1.9352 - acc: 0.8451 - val_loss: 2.7573 - val_acc: 0.7884\nEpoch 28/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 1.7868 - acc: 0.8596 - val_loss: 2.8335 - val_acc: 0.7799\nEpoch 29/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 1.9218 - acc: 0.8519 - val_loss: 2.9394 - val_acc: 0.7986\nEpoch 30/30\n2343/2343 [==============================] - 7s 3ms/step - loss: 1.8047 - acc: 0.8613 - val_loss: 2.7308 - val_acc: 0.7884\ntraining time: -220.9209759235382\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(loss, accuracy)=custom_vgg_model.evaluate(test,y_test_dummies,verbose=1 ,batch_size=32)\nprint(loss)\nprint(accuracy*100)","execution_count":165,"outputs":[{"output_type":"stream","text":"733/733 [==============================] - 2s 2ms/step\n3.2996104137634026\n73.2605730202481\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted=[]\nsubmit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nfor i, name in tqdm(enumerate(submit['id_code'])):\n    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n    image = cv2.imread(path)\n    image = cv2.resize(image, (224, 224))\n    score_predict = model.predict((image[np.newaxis])/255)\n    label_predict = np.argmax(score_predict)\n    # label_predict = score_predict.astype(int).sum() - 1\n    predicted.append(str(label_predict))","execution_count":166,"outputs":[{"output_type":"stream","text":"1928it [00:56, 34.28it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['diagnosis'] = predicted\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","execution_count":167,"outputs":[{"output_type":"execute_result","execution_count":167,"data":{"text/plain":"        id_code diagnosis\n0  0005cfc8afb6       669\n1  003f0afdcd15       669\n2  006efc72b638       669\n3  00836aaacf06       669\n4  009245722fa4       669","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0005cfc8afb6</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>003f0afdcd15</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>006efc72b638</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>00836aaacf06</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>009245722fa4</td>\n      <td>669</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}