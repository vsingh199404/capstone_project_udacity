{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nSEED=2\nimport os\n\ntrain=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/train.csv\")\nsubmition=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/test.csv\")\n\nx = train['id_code']\ny = train['diagnosis']\n\nx,y=shuffle(x,y)\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X,X_test,df_y,y_test=train_test_split(x, y, test_size=0.15)\n\nX_train,X_valid,y_train,y_valid=train_test_split(df_X, df_y, test_size=0.15)\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_ben_color(path, sigmaX=20):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n        \n    return image","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\ntrain_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_train):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    train_images.append(image)\n\n        ","execution_count":5,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 9.54 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\ntest_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_test):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    test_images.append(image)\n","execution_count":6,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 7.39 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\nvalid_images=[]\nIMG_SIZE=224\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_valid):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    valid_images.append(image)","execution_count":7,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 8.82 µs\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\nNUM_CLASSES=5","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ny_train_dummies=to_categorical(y_train,num_classes=NUM_CLASSES)\ny_test_dummies=to_categorical(y_test,num_classes=NUM_CLASSES)\ny_valid_dummies=to_categorical(y_valid,num_classes=NUM_CLASSES)\ny_train_dummies","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([[0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       ...,\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport time\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers import merge, Input\nfrom keras.models import Model\nfrom keras.utils import np_utils\nimport os\n\n\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_Classes=5\n\ntrain=np.array(train_images)\ntest=np.array(test_images)\nvalid=np.array(valid_images)\n\nnum_of_samples=train.shape[0]\ntrain.shape","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"(2645, 224, 224, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n# create and configure augmented image generator\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n    horizontal_flip=True) # randomly flip images horizontally\n\n# create and configure augmented image generator\ndatagen_valid = ImageDataGenerator(\n    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n    horizontal_flip=True) # randomly flip images horizontally\n\n# fit augmented image generator on data\ndatagen_train.fit(train)\ndatagen_valid.fit(valid)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# take subset of training data\nx_train_subset = train[:12]\nfig = plt.figure(figsize=(20,2))\nfor x_batch in datagen_train.flow(x_train_subset, batch_size=12):\n    for i in range(0, 12):\n        ax = fig.add_subplot(1, 12, i+1)\n        ax.imshow(x_batch[i])\n    fig.suptitle('Augmented Images', fontsize=20)\n    plt.show()\n    break;","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1440x144 with 12 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image                  \nimage_input=Input(shape=train.shape[1:])\nmodel=VGG16(input_tensor=image_input, weights='imagenet')\nmodel.summary()","execution_count":33,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 20,878,312\nNon-trainable params: 117,479,232\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlast_layer=model.get_layer('fc1').output\nlast_layer=Dropout(0.2)(last_layer)\n\nlast_layer=Dense(1024)(last_layer)\nlast_layer=BatchNormalization()(last_layer)\nlast_layer=Activation('relu')(last_layer)\nlast_layer=last_layer=Dropout(0.2)(last_layer)\nlast_layer=Dense(512, activation='relu')(last_layer)\nlast_layer=BatchNormalization()(last_layer)\nlast_layer=Activation('relu')(last_layer)\nlast_layer=Dropout(0.2)(last_layer)\n\nout=Dense(num_Classes,activation='softmax', name='output')(last_layer)\n\ncustom_vgg_model=Model(image_input,out)\ncustom_vgg_model.summary()","execution_count":35,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1024)              4195328   \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 1024)              4096      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 1024)              0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 512)               524800    \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 512)               2048      \n_________________________________________________________________\nactivation_4 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 512)               0         \n_________________________________________________________________\noutput (Dense)               (None, 5)                 2565      \n=================================================================\nTotal params: 122,208,069\nTrainable params: 4,725,765\nNon-trainable params: 117,482,304\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in custom_vgg_model.layers[:-10]:\n    layer.trainable=False","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_vgg_model.compile(loss=\"categorical_crossentropy\", optimizer='Adam',  metrics=['accuracy'])\n","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt=time.time()\n\ncustom_vgg_model.fit_generator(datagen_train.flow(train, y_train_dummies, batch_size=batch_size),\n                    steps_per_epoch=train.shape[0]/32,\n                    epochs=2, verbose=2,\n                    validation_data=datagen_valid.flow(valid, y_valid_dummies, batch_size=batch_size),validation_steps=valid.shape[0]/32)\nprint(\"training time: %s\" %(t-time.time()))","execution_count":39,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n - 35s - loss: 0.4052 - acc: 0.8456 - val_loss: 0.6293 - val_acc: 0.7537\nEpoch 2/2\n - 34s - loss: 0.3727 - acc: 0.8610 - val_loss: 0.7088 - val_acc: 0.7623\ntraining time: -69.50976991653442\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 100\n\n\nt=time.time()\n\ncustom_vgg_model.fit_generator(datagen_train.flow(train, y_train_dummies, batch_size=batch_size),\n                    steps_per_epoch=train.shape[0]/16,\n                    epochs=40, verbose=2,\n                    validation_data=datagen_valid.flow(valid, y_valid_dummies, batch_size=batch_size),validation_steps=valid.shape[0]/16)\nprint(\"training time: %s\" %(t-time.time()))","execution_count":40,"outputs":[{"output_type":"stream","text":"Epoch 1/40\n - 73s - loss: 0.3862 - acc: 0.8500 - val_loss: 0.6768 - val_acc: 0.7752\nEpoch 2/40\n - 68s - loss: 0.3836 - acc: 0.8554 - val_loss: 0.6409 - val_acc: 0.7730\nEpoch 3/40\n - 68s - loss: 0.3562 - acc: 0.8657 - val_loss: 0.7105 - val_acc: 0.7709\nEpoch 4/40\n - 67s - loss: 0.3499 - acc: 0.8679 - val_loss: 0.6904 - val_acc: 0.7816\nEpoch 5/40\n - 69s - loss: 0.3508 - acc: 0.8602 - val_loss: 0.6379 - val_acc: 0.7762\nEpoch 6/40\n - 68s - loss: 0.3379 - acc: 0.8685 - val_loss: 0.7589 - val_acc: 0.7591\nEpoch 7/40\n - 68s - loss: 0.3443 - acc: 0.8641 - val_loss: 0.7082 - val_acc: 0.7762\nEpoch 8/40\n - 68s - loss: 0.3231 - acc: 0.8717 - val_loss: 0.6380 - val_acc: 0.7901\nEpoch 9/40\n - 68s - loss: 0.3172 - acc: 0.8795 - val_loss: 0.6778 - val_acc: 0.7602\nEpoch 10/40\n - 69s - loss: 0.3149 - acc: 0.8859 - val_loss: 0.7630 - val_acc: 0.7602\nEpoch 11/40\n - 67s - loss: 0.3235 - acc: 0.8764 - val_loss: 0.6993 - val_acc: 0.7623\nEpoch 12/40\n - 68s - loss: 0.2889 - acc: 0.8912 - val_loss: 0.7489 - val_acc: 0.7719\nEpoch 13/40\n - 68s - loss: 0.2889 - acc: 0.8848 - val_loss: 0.7857 - val_acc: 0.7377\nEpoch 14/40\n - 69s - loss: 0.3025 - acc: 0.8832 - val_loss: 0.7758 - val_acc: 0.7784\nEpoch 15/40\n - 68s - loss: 0.2811 - acc: 0.8949 - val_loss: 0.7125 - val_acc: 0.7698\nEpoch 16/40\n - 68s - loss: 0.2748 - acc: 0.8944 - val_loss: 0.7716 - val_acc: 0.7719\nEpoch 17/40\n - 67s - loss: 0.2664 - acc: 0.8985 - val_loss: 0.8464 - val_acc: 0.7559\nEpoch 18/40\n - 67s - loss: 0.2852 - acc: 0.8921 - val_loss: 0.7527 - val_acc: 0.7655\nEpoch 19/40\n - 68s - loss: 0.2680 - acc: 0.9022 - val_loss: 0.7471 - val_acc: 0.7837\nEpoch 20/40\n - 68s - loss: 0.2648 - acc: 0.8998 - val_loss: 0.7841 - val_acc: 0.7923\nEpoch 21/40\n - 68s - loss: 0.2396 - acc: 0.9113 - val_loss: 0.7752 - val_acc: 0.7709\nEpoch 22/40\n - 68s - loss: 0.2557 - acc: 0.9017 - val_loss: 0.8445 - val_acc: 0.7709\nEpoch 23/40\n - 69s - loss: 0.2391 - acc: 0.9098 - val_loss: 0.8239 - val_acc: 0.7805\nEpoch 24/40\n - 68s - loss: 0.2351 - acc: 0.9130 - val_loss: 0.8219 - val_acc: 0.7623\nEpoch 25/40\n - 68s - loss: 0.2411 - acc: 0.9089 - val_loss: 0.8327 - val_acc: 0.7762\nEpoch 26/40\n - 68s - loss: 0.2383 - acc: 0.9100 - val_loss: 0.7612 - val_acc: 0.7687\nEpoch 27/40\n - 69s - loss: 0.2199 - acc: 0.9165 - val_loss: 0.8658 - val_acc: 0.7944\nEpoch 28/40\n - 68s - loss: 0.2232 - acc: 0.9203 - val_loss: 0.7943 - val_acc: 0.7612\nEpoch 29/40\n - 68s - loss: 0.2158 - acc: 0.9153 - val_loss: 0.8668 - val_acc: 0.7794\nEpoch 30/40\n - 67s - loss: 0.2074 - acc: 0.9229 - val_loss: 0.8278 - val_acc: 0.7741\nEpoch 31/40\n - 68s - loss: 0.2227 - acc: 0.9200 - val_loss: 0.8110 - val_acc: 0.7709\nEpoch 32/40\n - 69s - loss: 0.2154 - acc: 0.9219 - val_loss: 0.8892 - val_acc: 0.7805\nEpoch 33/40\n - 68s - loss: 0.2079 - acc: 0.9200 - val_loss: 0.8724 - val_acc: 0.7580\nEpoch 34/40\n - 68s - loss: 0.2089 - acc: 0.9212 - val_loss: 0.8324 - val_acc: 0.7794\nEpoch 35/40\n - 68s - loss: 0.2048 - acc: 0.9242 - val_loss: 0.8484 - val_acc: 0.7848\nEpoch 36/40\n - 68s - loss: 0.1914 - acc: 0.9264 - val_loss: 0.8434 - val_acc: 0.7901\nEpoch 37/40\n - 68s - loss: 0.1944 - acc: 0.9286 - val_loss: 0.8692 - val_acc: 0.7655\nEpoch 38/40\n - 68s - loss: 0.1887 - acc: 0.9266 - val_loss: 0.9494 - val_acc: 0.7463\nEpoch 39/40\n - 68s - loss: 0.1928 - acc: 0.9236 - val_loss: 0.8290 - val_acc: 0.7773\nEpoch 40/40\n - 68s - loss: 0.1841 - acc: 0.9326 - val_loss: 0.9053 - val_acc: 0.7859\ntraining time: -2723.6046426296234\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nt=time.time()\n\ncustom_vgg_model.fit(train,y_train_dummies, batch_size=32, epochs=30,verbose=1 ,validation_data=(valid,y_valid_dummies))\n\nprint(\"training time: %s\" %(t-time.time()))","execution_count":21,"outputs":[{"output_type":"stream","text":"Train on 2645 samples, validate on 467 samples\nEpoch 1/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.9699 - acc: 0.7905 - val_loss: 2.5098 - val_acc: 0.7773\nEpoch 2/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.7895 - acc: 0.8178 - val_loss: 2.4016 - val_acc: 0.7794\nEpoch 3/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.6607 - acc: 0.8257 - val_loss: 2.3419 - val_acc: 0.7859\nEpoch 4/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.7067 - acc: 0.8223 - val_loss: 2.2510 - val_acc: 0.7666\nEpoch 5/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.5239 - acc: 0.8329 - val_loss: 2.1065 - val_acc: 0.7773\nEpoch 6/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.4916 - acc: 0.8378 - val_loss: 2.1745 - val_acc: 0.7837\nEpoch 7/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.5039 - acc: 0.8423 - val_loss: 2.2255 - val_acc: 0.7816\nEpoch 8/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3883 - acc: 0.8601 - val_loss: 2.0952 - val_acc: 0.7966\nEpoch 9/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2684 - acc: 0.8662 - val_loss: 2.1072 - val_acc: 0.7901\nEpoch 10/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3200 - acc: 0.8612 - val_loss: 2.1092 - val_acc: 0.7816\nEpoch 11/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3526 - acc: 0.8616 - val_loss: 2.4776 - val_acc: 0.7773\nEpoch 12/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3652 - acc: 0.8582 - val_loss: 2.2039 - val_acc: 0.7859\nEpoch 13/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2688 - acc: 0.8722 - val_loss: 2.1644 - val_acc: 0.7794\nEpoch 14/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2029 - acc: 0.8798 - val_loss: 2.4261 - val_acc: 0.7859\nEpoch 15/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3315 - acc: 0.8624 - val_loss: 2.2457 - val_acc: 0.7752\nEpoch 16/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2827 - acc: 0.8696 - val_loss: 2.5034 - val_acc: 0.7302\nEpoch 17/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2735 - acc: 0.8635 - val_loss: 2.1454 - val_acc: 0.7923\nEpoch 18/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2767 - acc: 0.8669 - val_loss: 2.4046 - val_acc: 0.7602\nEpoch 19/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2129 - acc: 0.8767 - val_loss: 2.4711 - val_acc: 0.7859\nEpoch 20/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2334 - acc: 0.8749 - val_loss: 2.4286 - val_acc: 0.7580\nEpoch 21/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.1825 - acc: 0.8805 - val_loss: 2.3105 - val_acc: 0.7709\nEpoch 22/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2267 - acc: 0.8779 - val_loss: 2.3318 - val_acc: 0.7730\nEpoch 23/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.1507 - acc: 0.8892 - val_loss: 2.4881 - val_acc: 0.7709\nEpoch 24/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2462 - acc: 0.8767 - val_loss: 2.2203 - val_acc: 0.7773\nEpoch 25/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.3172 - acc: 0.8699 - val_loss: 3.0665 - val_acc: 0.7816\nEpoch 26/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.2424 - acc: 0.8794 - val_loss: 2.3801 - val_acc: 0.7666\nEpoch 27/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.1785 - acc: 0.8904 - val_loss: 2.2569 - val_acc: 0.7880\nEpoch 28/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.1383 - acc: 0.8866 - val_loss: 2.4193 - val_acc: 0.7730\nEpoch 29/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.1466 - acc: 0.8904 - val_loss: 2.3544 - val_acc: 0.7666\nEpoch 30/30\n2645/2645 [==============================] - 8s 3ms/step - loss: 1.0909 - acc: 0.8960 - val_loss: 2.2203 - val_acc: 0.7709\ntraining time: -234.2395350933075\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(loss, accuracy)=custom_vgg_model.evaluate(test,y_test_dummies,verbose=1 ,batch_size=32)\nprint(loss)\nprint(accuracy*100)","execution_count":41,"outputs":[{"output_type":"stream","text":"550/550 [==============================] - 1s 3ms/step\n0.9775662539222024\n75.09090906923468\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\npredicted=[]\nsubmit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nfor i, name in tqdm(enumerate(submit['id_code'])):\n    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n    \n    image = load_ben_color(path)\n    image = cv2.resize(image, (224, 224))\n    score_predict = model.predict((image[np.newaxis])/255)\n    label_predict = np.argmax(score_predict)\n    #label_predict = score_predict.astype(int).sum() - 1\n    predicted.append(str(label_predict))","execution_count":42,"outputs":[{"output_type":"stream","text":"1928it [03:24,  9.44it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['diagnosis'] = predicted\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"        id_code diagnosis\n0  0005cfc8afb6       669\n1  003f0afdcd15       669\n2  006efc72b638       669\n3  00836aaacf06       669\n4  009245722fa4       669","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_code</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0005cfc8afb6</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>003f0afdcd15</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>006efc72b638</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>00836aaacf06</td>\n      <td>669</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>009245722fa4</td>\n      <td>669</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
